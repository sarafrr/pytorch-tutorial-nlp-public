{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AutoGrad: Automatic Differentiation in PyTorch\n",
        "\n",
        "This tutorial covers PyTorch [autograd](https://pytorch.org/docs/stable/autograd.html) implementation of gradient descent.\n",
        "\n",
        "Previously, we performed several operations on tensors, but we did nothing to store the sequences of operations or to perform the derivatives of a function.\n",
        "\n",
        "Here, we introduce the concept of *dynamical computational graph* which comprises all the Tensor objects in the network, as well as the *Functions* used to create them.\n",
        "\n",
        "The PyTorch autograd package provides automatic differentiation for all operations on Tensors, indeed operations become attributes of the tensors. When a tensor's .requires_grad attribute is set to True, it starts to track all operations on it. When you want to compute the gradients on the ''final tensor'', you can call `backward()` and all the gradients are computed.\n",
        "The gradient of a tensor will be accumulated in the `.grad` attribute."
      ],
      "metadata": {
        "id": "9NkqNkex3rDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "afeV6sNb3qh1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x = torch.tensor(1.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back-propagation on one step\n",
        "We apply a polynomial function $y = f(x)$ to tensor $x$. Then we will backprop and print the gradient $\\frac {dy} {dx}$.\n",
        "\n",
        "$\\begin{split}Function:\\quad y &= x^4 + 3x^3 + 3x^2 + 5x + 1 \\\\\n",
        "Derivative:\\quad y' &= 4x^3 + 9x^2 + 6x + 5\\end{split}$"
      ],
      "metadata": {
        "id": "fxj5ni8VNToY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**4 + 3*x**3 + 3*x**2 + 5*x + 1\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbfrlwE6NhwT",
        "outputId": "24203e22-2db4-4cde-b5f2-730ba2986534"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(13., grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since $y$ has been computed as a result of an operation, it has an associated gradient function, accessible as `y.grad_fn`."
      ],
      "metadata": {
        "id": "Q_V3yc52NsqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "0NIAKNwDOU1G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLymfc8ZOWsg",
        "outputId": "1eb5446b-6fcf-497b-9904-9ae5b7d94982"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(24.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the computation of the derivative when x=1\n",
        "# 24 is the slope of the function at (x,y) = (1,13)\n",
        "print(4*1 + 9*1 + 6*1 + 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_hfou24OdZQ",
        "outputId": "aafba9c7-efff-4ab6-8a23-40f8de448d67"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back-propagation on multiple steps\n",
        "Let's have layers $y$ and $z$ after $x$."
      ],
      "metadata": {
        "id": "xVymcmCXPC6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYwQlNRYPQQs",
        "outputId": "14cd475c-dc77-46cd-95b7-4142df53ebcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [3., 2., 1.]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the first layer with $y = 2x+1$"
      ],
      "metadata": {
        "id": "ZXWqTbtGPTLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2*x + 1\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIcBAcEQPany",
        "outputId": "ea99c1b5-ff44-4ca0-f535-bf261d677b50"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 5., 7.],\n",
            "        [7., 5., 3.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the second layer $z=y^2$"
      ],
      "metadata": {
        "id": "_p2yv4oCPgFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = y**2\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYmz8BsOPkyJ",
        "outputId": "d0ffd682-8d74-4879-fe5f-788d79d79c31"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9., 25., 49.],\n",
            "        [49., 25.,  9.]], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the output to be the matrix mean."
      ],
      "metadata": {
        "id": "NfwTMCdvPptq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o = z.mean()#sum of all the elements divided by the number of elements\n",
        "print(o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrROq1jyPuvo",
        "outputId": "fe070936-c1b1-4f97-99a4-8010119c4904"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(27.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we compute the gradient of $x$ w.r.t. $out$."
      ],
      "metadata": {
        "id": "p5AP-tWWP5ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ikCObUP2x6",
        "outputId": "4a3691d0-cb0e-41a9-c676-404fdf8e3981"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 3.3333, 4.6667],\n",
            "        [4.6667, 3.3333, 2.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see a 2x3 matrix. We can calculate the partial derivative of $o$ with respect to $x_i$ as follows:<br>\n",
        "\n",
        "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
        "\n",
        "$z_i = (y_i)^2 = (2x_i+1)^2$<br>\n",
        "\n",
        "To solve the derivative of $z_i$ we use the <a href='https://en.wikipedia.org/wiki/Chain_rule'>chain rule</a>, where the derivative of $f(g(x)) = f'(g(x))g'(x)$<br>\n",
        "\n",
        "In this case<br>\n",
        "\n",
        "$\\begin{split} f(g(x)) &= (g(x))^2, \\quad &f'(g(x)) = 2g(x) \\\\\n",
        "g(x) &= 2x+1, &g'(x) = 2 \\\\\n",
        "\\frac {dz} {dx} &= 4g(x) &= 4(2x+1) \\end{split}$\n",
        "\n",
        "Therefore,<br>\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 4(2x+1)$<br>\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{2}{3}(2(1)+1) = 2$\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = \\frac{2}{3}(2(2)+1) = 3.333333333$\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = \\frac{2}{3}(2(3)+1) = 4.666666666$"
      ],
      "metadata": {
        "id": "-yASOuXbQG7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(2/3*3)\n",
        "print(2/3*5)\n",
        "print(2/3*7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyB7hFntR96C",
        "outputId": "bd36bad2-ccd3-4e90-e7f9-39e065b0ec50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n",
            "3.333333333333333\n",
            "4.666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn off tracking\n",
        "There may be times when we don't want or need to track the computational history.\n",
        "\n",
        "You can reset a tensor's <tt>requires_grad</tt> attribute in-place using `.requires_grad_(False)`.\n",
        "\n",
        "When performing evaluations, it's often helpful to wrap a set of operations in `with torch.no_grad():`\n",
        "\n",
        "It is also possible to use `.detach()` on a tensor to not track future computations."
      ],
      "metadata": {
        "id": "66-N6xhESqj5"
      }
    }
  ]
}