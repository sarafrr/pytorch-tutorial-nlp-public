{"cells":[{"cell_type":"markdown","metadata":{"id":"H6H6ieIdVQ56"},"source":["# General Note\n","In order to guide you through the homework, we put \"...COMPLETE HERE...\" as placeholder for you to complete the homework.\n","\n","# Disclaimer\n","Here, we will perform the training without validation. This is not the correct way to\n","decide for the best model, but it helps in the visualisation part.  \n","To be in overfitting is fine with us in this toy problem.  \n","The goal here is to understand if you are able to implement the two classes and  \n","see that the results have a meaning.\n","\n","# Update\n","Download again the data as the new data are cleaned by the initial part which was\n","containing also the license of the textual files."]},{"cell_type":"markdown","metadata":{"id":"4i8ua3T7fN-b"},"source":["# Word Embeddings\n","Word embeddings are *dense vectors of real numbers* (they have tipically entries that are non-zero). One dense vector per element in a dictionary.\n","In Natural Language Processing (NLP) you deal with words.\n","\n","Thus, you have to represent words in a computer, how to do it?\n","It is possible to use the ASCII representation of the characters composing the word.\n","\n","However, this kind of representation does not tell you anything about its meaning, but only about what it is exactly.\n","\n","Generally you want to obtain a sort of representation of your high-dimensional data to a smaller representation.\n","\n","You can also usa the so-called ''one-hot encoding'' of the word in a vocabulary. Let's represent the word $w$ in a vocabulary of $V$ words:\n","\n","\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n","\n","where we have $1$ at the position of $w$.\n","\n","There are different drawbacks in representing the word like that. It could be an enormous representation, but also it is considering the words as independent. We would like to have a representation of the words that contains a sort of notion of *similarity*.\n","\n","Let's see why.\n","\n","Suppose we want to build a language model, and that we have seen the following sentences:\n","\n","\n","*   the girl ran to the shop\n","*   the boy ran to the shop\n","*   the girl bought a flower\n","\n","in the training data. Then, suppose we see sentences never seen before in our training data\n","\n","*   the boy bought a flower\n","\n","Maybe, the language model would tell that it is probable the previous sentence. But what about using the following information we have seen in our training data:\n","*  we have seen the boy and the girl in the same role in the sentence, thus maybe they share some semantic information\n","*  we have seen the girl in the same relation of the boy we are seeing now\n","\n","These two facts shold increase the probability of correctness of the sentences.\n","We mean this for *semantic similarity*. This is a technique to combat the sparsity of the linguistic data.\n","\n","This representation however rely on the assumption that words appearning in similar contexts are related to each other semantically.\n","\n","\n","## Getting Dense Word Embeddings\n","How could we encode semantic similarity in words? Maybe, we can introduce some semantic attributes.\n","\n","For example, we know that both the boy and girl can run. Thus, we can give an high score to the attribute \"is able to run\". We can think other semantic attributes and in this way we construct the embeddings of the two subjects: the boy and the girl.\n","\n","\\begin{align}q_\\text{boy} = \\left[ \\overbrace{2.3}^\\text{can run},\n","   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{likes beach}, \\dots \\right]\\end{align}\n","\n","\\begin{align}q_\\text{girl} = \\left[ \\overbrace{2.5}^\\text{can run},\n","   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{likes beach}, \\dots \\right]\\end{align}\n","\n","We can measure the similarity between the boy and girl by doing a dot-product between the two representations:\n","\n","\\begin{align}\\text{Similarity}(\\text{boy}, \\text{girl}) = q_\\text{boy} \\cdot q_\\text{girl}\\end{align}\n","\n","Although it is more common to normalize by the lengths:\n","\n","\\begin{align}\\text{Similarity}(\\text{boy}, \\text{girl}) = \\frac{q_\\text{boy} \\cdot q_\\text{girl}}\n","   {\\| q_\\text{boy} \\| \\| q_\\text{girl} \\|} = \\cos (\\phi)\\end{align}\n","\n","Where $\\phi$ is the angle between the two vectors. That way,\n","extremely similar words (words whose embeddings point in the same\n","direction) will have similarity 1. Extremely dissimilar words should\n","have similarity -1.\n","\n","\n","We let the network learn this semantic attributes, by definining them as parameters. We will have some *latent semantic attributes* that the network can, in principle, learn. Note that the word embeddings will probably not be interpretable.\n","\n","In summary, **word embeddings are a representation of the *semantics* of\n","a word, efficiently encoding semantic information that might be relevant\n","to the task at hand**.\n","\n","## Word Embeddings in Pytorch\n","\n","Similarly to what we do with the one-hot-encoding representation, we have to define an index for each word in the vocabulary. These will be the keys in a lookup table. That is embeddings are stored in a $|V| \\times D$ matrix, were $D$ is the dimensionality of the embeddings, such that the word assigned inex $i$ has the embedding stored in the row of the matrix of index $i$.\n","\n","The module that permits you to do the embeddings is torch.nn.Embedding which takes to parameters: the dimensionality of the vocabulary and the dimension of the embeddings.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"FILi19R_rrBH"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'spacy'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19616\\3447510594.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random\n","\n","import spacy\n","import nltk\n","from nltk.corpus import stopwords\n","\n","import string\n","import re\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVmcI1m6FP-4"},"outputs":[],"source":["# this is all the necessary code to set the seed\n","def set_seed(seed : int = 123):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HRsafGhFdK-"},"outputs":[],"source":["set_seed()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0p26mHrrxZx"},"outputs":[],"source":["word_to_ix = {\"hello\": 0, \"world\": 1}\n","\n","# rows = n. words in the vocabulary\n","# cols = dimension of the embedding\n","embeds = nn.Embedding(2, 5)\n","# tensor to store the index to access the embedding of a word\n","word = 'hello'\n","# word = 'world'\n","lookup_tensor = torch.tensor([word_to_ix[word]], dtype=torch.long)\n","\n","word_embed = embeds(lookup_tensor)\n","print(word_embed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnawCyXitAeR"},"outputs":[],"source":["# load google drive to see the files in google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyHt25uFCn-1"},"outputs":[],"source":["!git clone https://github.com/sarafrr/pytorch-tutorial-nlp-public.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfBqSjz_Cq5s"},"outputs":[],"source":["!rm -r pytorch-tutorial-nlp-public"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlGIGTaxDLRr"},"outputs":[],"source":["# from the folder containing the data, obtain the list of all the files\n","from glob import glob\n","file_list = glob( \"/content/data/Shakespeare/*.txt\")\n","\n","print(file_list)"]},{"cell_type":"markdown","metadata":{"id":"t-6tHBiFBtv8"},"source":["# Exercise: Compute the Word Embedding Continuous Bag-of-Words\n","\n","The Continuous Bag-of-Words (CBOW) is widely used in NLP, it is a way to embed the context of few words before and after of the target word.\n","\n","CBOW is used to train word embeddings and these embeddings are used as initialisation of a more complicate model.\n","\n","This is usually referred as *pretraining embeddings*.\n","\n","Given a target word $w_i$ and an\n","$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n","and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n","collectively as $C$, CBOW tries to minimize\n","\n","\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right)\\end{align}\n","\n","where $q_w$ is the embedding for word $w$.\n","\n","\n","Implement the CBOW class in the following snippet of code and train it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb9_VJd8lbY3"},"outputs":[],"source":["# CONSTANTS - do not modify this cell of code\n","\n","# 2 words to the left and two to the right\n","CONTEXT_SIZE = 2\n","EMBEDDING_SIZE = 100\n","EPOCHS = 50\n","LEARNING_RATE = 1e-3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VXbmwGASyD5"},"outputs":[],"source":["# download stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J297rs29Svvk"},"outputs":[],"source":["stop_words = set(stopwords.words('english'))# e.g.: a, in, is\n","print(stop_words)\n","punctuation = set(string.punctuation)\n","print(punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nldwyv4DDZ2w"},"outputs":[],"source":["def prepare_corpus(file_list : list, min_size : int = 1):\n","  ''' Read and prepare the corpus as a list of sentences '''\n","  # retrieve all the text from the files\n","  # a string we named complete_text\n","  complete_text = ''\n","  for p in file_list:\n","    with open(p, 'r') as f:\n","      lines = f.readlines()\n","      for l in lines:\n","        complete_text += l\n","  # retrieve the sentences by the complete_text\n","  sentences = re.split('[.!?]', complete_text)\n","\n","  # filter the sentences by the number of words\n","  corpus = []\n","  # sentences is a list of strings\n","  for s in sentences:\n","    s = s.replace('\\n', ' ')\n","    if len(s.split()) > min_size*2 + 1:\n","      corpus.append(s)\n","  return corpus\n","\n","corpus = prepare_corpus(file_list=file_list, min_size = 2)\n","print(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4zI7dfcr4Y2"},"outputs":[],"source":["def prepare_data(corpus : list, context_size : int):\n","  ''' Tokenizes the data, removing the stop words and the punctuation.\n","      Creates and returns data composed by the tuple of the context and the target,\n","      and the dictionaries\n","  '''\n","  voc = set()\n","  tokenized_corpus = []\n","  for i in corpus:\n","    tokenized = nltk.word_tokenize(i)\n","    # make capital letters lowercase\n","    tokenized = [x.lower() for x in tokenized]\n","    # remove stopwords and punctuation\n","    tokenized = [x for x in tokenized if x not in (stop_words|punctuation)]\n","    tokenized_corpus.append(tokenized)\n","\n","    voc |= set(tokenized)\n","\n","  w_to_i = dict(zip(voc,range(len(voc))))\n","  i_to_w = dict(zip(range(len(voc)),voc))\n","\n","  # create the data\n","  data = []\n","  for s in tokenized_corpus:\n","    for i in range(context_size, len(s)-context_size):\n","      context_idx = [w_to_i[s[i-j-1]] for j in range(context_size)]+ \\\n","              [w_to_i[s[i+j+1]] for j in range(context_size)]\n","      target_idx = w_to_i[s[i]]\n","      data.append((context_idx, target_idx))\n","\n","  return data, w_to_i, i_to_w"]},{"cell_type":"markdown","metadata":{"id":"MCdyF5HBVINt"},"source":["# Question 1\n","\n","Implement CBOW model by defining an appropriate class which extends `nn.Module` and train it on the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpKprQAeDQrn"},"outputs":[],"source":["# Create the model\n","class CBOW(nn.Module):\n","\n","    def __init__(self, ''' COMPLETE HERE '''):\n","        super(CBOW, self).__init__()\n","        ''' COMPLETE HERE '''\n","\n","    def forward(self, x):\n","        ''' COMPLETE HERE '''\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"NWVXOGNEVKwM"},"source":["# Question 2\n","Try to do the appropriate modifications to this notebook to use the GPU. (Tip: see the introductory notebooks about PyTorch, specifically 2_Introduction_to_Pytorch.ipynb or see the notebook NLP_homework1_skipgram(hw).ipynb.)\n","\n","# Question 3\n","Plot one sample of the dataset: context, target and the respective prediction and see that the prediction is equal to the target. See the last lines of code of the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqPGzXCvxf54"},"outputs":[],"source":["data, w_to_i, i_to_w = prepare_data(corpus, context_size = CONTEXT_SIZE)\n","print(len(w_to_i))\n","# implement the loss\n","# justify the use of this loss\n","criterion = nn.CrossEntropyLoss()\n","model = CBOW(''' COMPLETE HERE ''')\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# create the context data and targets in tensors\n","context_data = torch.tensor([d[0] for d in data])\n","target_data = torch.tensor([d[1] for d in data])\n","\n","# create a dataset and a dataloader\n","dataset = torch.utils.data.TensorDataset(context_data, target_data)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","for e in range(EPOCHS):\n","  for idx, data in enumerate(dataloader):\n","    context = data[0]\n","    target = data[1]\n","    predicted = model(context)\n","    loss = criterion(predicted, target)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","  print(f'Epoch {e} - loss: {loss.item()}')\n","\n","# --- TESTING ---\n","data, w_to_i, i_to_w = prepare_data(corpus, context_size = CONTEXT_SIZE)\n","# create the context data and targets in tensors\n","context_data = torch.tensor([d[0] for d in data])\n","target_data = torch.tensor([d[1] for d in data])\n","\n","# create a dataset and a dataloader\n","dataset = torch.utils.data.TensorDataset(context_data, target_data)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","for idx, data in enumerate(dataloader):\n","\n","  context = data[0]\n","  target = data[1]\n","  out = model(context)\n","  break\n","\n","#print result\n","print(f'Context: {''' COMPLETE HERE '''}')\n","print(f'Target: {''' COMPLETE HERE '''}')\n","print(f'Prediction: {''' COMPLETE HERE '''}')\n"]},{"cell_type":"markdown","metadata":{"id":"rChqhyt_XWy3"},"source":["# Question 4 (Updated)\n","Plot the original embeddings and trained ones and explain the difference between the two.\n","\n","To do so use the function `tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3)` of the `scikit-learn` package.\n","\n","Indeed, using T-distributed Stochastic Neighbor Embedding (or other similar functions, such as the PCA), we can see the embeddings in 2D vector space. Explain the results considering the best plot you obtain by one of the two functions. If the plotting is not so explanatory,\n","feel free to use the cosine similarity or other techniques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP6-aGo0FqQb"},"outputs":[],"source":["for i,p in model.named_parameters():\n","  print(i)\n","\n","W = ''' COMPLETE HERE '''\n","print(W.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BsWo6kp0FZER"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","tsne = TSNE(n_components=2, learning_rate='auto',\n","                  init='random', perplexity=3)\n","W_dec = tsne.fit_transform(W)\n","x = ''' COMPLETE HERE '''\n","y = ''' COMPLETE HERE '''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taearbDAayYH"},"outputs":[],"source":["# check the dimensions\n","print(W_dec.shape)\n","print(x.shape)\n","print(y.shape)\n","print(len(i_to_w.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHE3qBUeZhJt"},"outputs":[],"source":["plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W_dec.shape[0]):\n","     plot.text(x[i], y[i], list(i_to_w.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"markdown","metadata":{"id":"9SQqjLYeTIlz"},"source":["Or using the PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCvukt3YTH3w"},"outputs":[],"source":["from sklearn import decomposition\n","\n","orig_model = CBOW(''' COMPLETE HERE ''')\n","W1_orig = '''COMPLETE HERE '''\n","\n","svd = decomposition.TruncatedSVD(n_components=2)\n","W_orig_dec = svd.fit_transform(W1_orig)\n","x = ''' COMPLETE HERE '''\n","y = ''' COMPLETE HERE '''\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W_orig_dec.shape[0]):\n","     plot.text(x[i], y[i], list(i_to_w.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}
