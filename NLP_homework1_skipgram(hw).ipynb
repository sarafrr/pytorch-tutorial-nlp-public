{"cells":[{"cell_type":"markdown","metadata":{"id":"qIUrIbQeyD7z"},"source":["# General Note \n","In order to guide you through the homework, we put \"...COMPLETE HERE...\" as placeholder for you to complete the homework."]},{"cell_type":"markdown","metadata":{"id":"4keMCu-heF2w"},"source":["# Word-embeddings\n","Word embeddings are very important in Natural Language Processing (NLP). Ready-to-use solutions (e.g., *Glove* and many others) are useful and relatively efficient. Skip-gram works empirically worse than others types of word-embeddings. Moreover, it needs much time to fit."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":604,"status":"ok","timestamp":1695804244343,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"JAfNtlvsdy1-"},"outputs":[],"source":["#https://pythonspot.com/nltk-stop-words/\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1695804230702,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"autJLdhvfIEw"},"outputs":[],"source":["corpus = [\n","    'Venice is a city in Veneto',\n","    'Padua is a city in Veneto',\n","    'Vicenza is a city in Veneto',\n","    'Verona is a city in Veneto',\n","    'Treviso is a city in Veneto',\n","    'Rovigo is a city in Veneto',\n","    'Bassano is in Veneto',\n","    'Chioggia is a city in Veneto',\n","    'Veneto is a region',\n","    'Bologna is a city in Emilia-Romagna',\n","    'Reggio-Emilia is in Emilia-Romagna',\n","    'Parma is a city in Emilia-Romagna',\n","    'Rimini is a city in Emilia-Romagna',\n","    'Ravenna is a city in Emilia-Romagna',\n","    'Piacenza is a city in Emilia-Romagna',\n","    'Emilia-Romagna is a region'\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1695804248240,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"vXjRZN5LwU3X"},"outputs":[],"source":["# this is all the necessary code to set the seed\n","def set_seed(seed : int = 123):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":379,"status":"ok","timestamp":1695804251172,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"JVs8qlYXwVa_"},"outputs":[],"source":["set_seed()"]},{"cell_type":"markdown","metadata":{"id":"-E-kSiqUfQ-A"},"source":["Skip-gram model tries to *predict context given a word*."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1695804256503,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"Cuz_nVYhjczQ"},"outputs":[],"source":["def w_to_i(text : list) -> dict:\n","  vocab = set()\n","  for l in text:\n","    words = l.split()\n","    vocab |= set(words)\n","\n","  vocab = dict(zip(vocab, range(0,len(vocab))))\n","  return vocab"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":437,"status":"ok","timestamp":1695804264061,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"CGvpT0pexS0l"},"outputs":[],"source":["def prepare_set(corpus, n_gram = 2):\n","    '''Creates a dataset with Input column and Output column for neighboring words.'''\n","    columns = ['Input', 'Output']\n","    result = pd.DataFrame(columns = columns)\n","    for sentence in corpus:\n","      words = sentence.split()\n","      for i,w in enumerate(words):\n","          inp = w\n","          for n in range(1,n_gram+1):\n","              # look back\n","              if (i-n)>=0:\n","                  out = words[i-n]\n","                  row = pd.DataFrame([[inp,out]], columns = columns)\n","                  result = pd.concat([result,row], axis = 0, ignore_index = True)\n","\n","              # look forward\n","              if (i+n)<len(words):\n","                  out = sentence.split()[i+n]\n","                  row = pd.DataFrame([[inp,out]], columns = columns)\n","                  result = pd.concat([result,row], axis = 0, ignore_index = True)\n","    return result"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1859,"status":"ok","timestamp":1695804271266,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"hEv5F15YBukf","outputId":"4aec6eef-46ab-4a87-c4aa-b7b0265f480f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["{'their', 'more', 'because', 'had', \"you'll\", 'is', 'was', 'weren', 'him', 'hers', 'most', 'whom', 'when', 'yourselves', 'myself', 'once', 'but', \"shan't\", 'herself', 'i', 'd', 've', 'she', 'll', 'some', 'didn', 'than', \"wouldn't\", 'after', \"you've\", 'are', 'my', 'ours', 'other', 'those', 'mustn', 'needn', 'does', 'this', \"weren't\", \"you'd\", 'each', 'do', 'very', 'been', 'what', 'hasn', 'any', 'why', 's', 'into', 'theirs', 'under', \"should've\", \"won't\", 'be', 'in', 'until', 'now', \"mustn't\", 'only', 'me', 'himself', \"it's\", 'for', 'her', 'has', 'its', 'from', 'by', \"that'll\", 'our', 'so', 'we', 'before', 'aren', 'all', 'such', 'again', 'haven', 'above', 'during', 'of', \"couldn't\", 'ourselves', 'between', \"shouldn't\", 'should', \"hadn't\", 'at', 'while', 'ain', \"isn't\", 'wouldn', 'down', \"doesn't\", 'there', 'couldn', 'through', 'below', 'over', 'themselves', 'he', 'm', 'isn', 'y', \"haven't\", 'did', 'the', 'them', 'having', 'up', 'too', 'further', 'hadn', 'won', 'which', 'on', 'an', 'few', 'o', 'same', 'and', 'here', \"don't\", 'don', 'no', \"mightn't\", 'his', 'you', \"didn't\", 'just', 'how', \"you're\", 'or', 'out', 'have', 'where', 'am', 'they', 't', 'against', 'itself', 'doing', 'as', 'not', 'your', 'off', \"wasn't\", \"she's\", \"hasn't\", 'it', \"aren't\", 'who', 'doesn', 'nor', 'a', 'these', 'about', 'if', 'yourself', 'were', 'shouldn', 'shan', 'mightn', 'ma', 'that', 're', 'both', 'then', 'being', \"needn't\", 'own', 'will', 'can', 'wasn', 'yours', 'with', 'to'}\n","Original corpus\n","['Venice is a city in Veneto', 'Padua is a city in Veneto', 'Vicenza is a city in Veneto', 'Verona is a city in Veneto', 'Treviso is a city in Veneto', 'Rovigo is a city in Veneto', 'Bassano is in Veneto', 'Chioggia is a city in Veneto', 'Veneto is a region', 'Bologna is a city in Emilia-Romagna']\n","Processed corpus\n","['venice city veneto', 'padua city veneto', 'vicenza city veneto', 'verona city veneto', 'treviso city veneto', 'rovigo city veneto', 'bassano veneto', 'chioggia city veneto', 'veneto region', 'bologna city emilia-romagna']\n"]}],"source":["# download stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","stop_words = set(stopwords.words('english'))# e.g.: a, in, is\n","print(stop_words)\n","\n","def preprocess(corpus):\n","    result = []\n","    for i in corpus:\n","        out = nltk.word_tokenize(i)\n","        # make capital letters lowercase\n","        out = [x.lower() for x in out]\n","        out = [x for x in out if x not in stop_words]\n","        result.append(\" \". join(out))\n","    return result\n","\n","processed_corpus = preprocess(corpus)\n","\n","print('Original corpus')\n","print(corpus[:10])\n","print('Processed corpus')\n","print(processed_corpus[:10])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1695804276140,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"pltnaiL5uNeI","outputId":"f56c69aa-3afd-4e90-a2b2-8005979a6c43"},"outputs":[{"name":"stdout","output_type":"stream","text":["The vocabulaty has dimension 18\n","{'veneto': 0, 'padua': 1, 'vicenza': 2, 'region': 3, 'city': 4, 'piacenza': 5, 'rimini': 6, 'rovigo': 7, 'treviso': 8, 'verona': 9, 'parma': 10, 'ravenna': 11, 'chioggia': 12, 'bassano': 13, 'venice': 14, 'emilia-romagna': 15, 'reggio-emilia': 16, 'bologna': 17}\n"]}],"source":["vocabulary = w_to_i(processed_corpus)\n","dim_vocabulary = len(vocabulary)\n","print(f'The vocabulaty has dimension {dim_vocabulary}')\n","print(vocabulary)"]},{"cell_type":"markdown","metadata":{"id":"oxxT3dcnj5DL"},"source":["# Question 1\n","Choose the more appropriate dimension for the context, that is the paramerer `n_gram` in the function `prepare_set()` and explain why you chose for that dimension."]},{"cell_type":"markdown","metadata":{"id":"vewpLhqKj4rW"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSyrEneBx_2x"},"outputs":[],"source":["# choose the more appropriate n_gram\n","n_gram = \"...COMPLETE HERE...\"\n","train_emb = prepare_set(processed_corpus, n_gram=n_gram)\n","train_emb.head(10)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":385,"status":"ok","timestamp":1695804305054,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"PcKOpBqH-K_C"},"outputs":[],"source":["# from words to indices\n","# function map to apply to Pandas DataFrames\n","train_emb.Input = train_emb.Input.map(vocabulary)\n","train_emb.Output = train_emb.Output.map(vocabulary)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1695804310131,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"uUXDkq-Gy2Az","outputId":"b74c5a06-8654-4208-ab80-9e406dc848fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Input  Output\n","0     14       4\n","1     14       0\n","2      4      14\n","3      4       0\n","4      0       4\n","5      0      14\n","6      1       4\n","7      1       0\n","8      4       1\n","9      4       0\n","torch.Size([80])\n"]}],"source":["print(train_emb.head(10))\n","\n","\n","for i,x in enumerate(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0])):\n","  print(x.shape)\n","  break"]},{"cell_type":"markdown","metadata":{"id":"IYF1nh8Px9z1"},"source":["In this way, we have represented through numbers both the input and the output. Then, we have to transform this representation of the input and the output with the one-hot-encoding representation.\n","\n","This means that the intput and the output have to be represented as vectors of the same dimension of the vocabulary, where all the elements in the vectors are zero, except for the one which represents the word.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":493,"status":"ok","timestamp":1695804315314,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"hZyDuUMi5Ki1"},"outputs":[],"source":["def get_input_tensor(indices : torch.LongTensor, vocab_size : int):\n","    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n","    size = indices.size(0)\n","    # batch size, vocab size\n","    one_hot_encoded_input = torch.zeros(size, vocab_size).scatter_(1, indices.unsqueeze(1), 1.)\n","    # print(one_hot_encoded_input)\n","    return one_hot_encoded_input"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":645,"status":"ok","timestamp":1695804318334,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"WIvWrPLg1z11","outputId":"faa0a0af-0104-46d1-f7f8-395ee98cad2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"]}],"source":["for i,x in enumerate(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0])):\n","  tmp = get_input_tensor(x, vocab_size=dim_vocabulary)\n","  break\n","# transform a tensor to a list\n","print(tmp.tolist())"]},{"cell_type":"markdown","metadata":{"id":"rC8KRY9ru_uy"},"source":["In between the input and output, there is an hidden layer we choose. The length of the hidden layer gives the dimension of the embedding vectors.\n","\n","The most interesting part of this network are the weights in between the hidden layer and the two other layers: the input and the output.\n","\n","The multiplication of the one-hot encoded vector with the matrix of weights will activate the only row which corresponds to the $1$ in the input vector.\n"]},{"cell_type":"markdown","metadata":{"id":"LZGXoDCOkqZ8"},"source":["# Question 2\n","Try to do the appropriate modifications to this notebook to use the GPU. (Tip: see the introductory notebooks about PyTorch, specifically 2_Introduction_to_Pytorch.ipynb.\n","\n","# Question 3\n","Implement SkipGram model by defining an appropriate class which extends `nn.Module` and train it on the data.\n","\n","# Question 4\n","Variate the embedding dimension in the range `[2:5]` and try to choose the best embedding dimension. Explain why you chose that dimension.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIPGHI6U5X3S"},"outputs":[],"source":["dim_embedding = \"...COMPLETE HERE...\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4M9GYeDvuYA"},"outputs":[],"source":["import torch.nn as nn\n","\n","class SkipGram(nn.Module):\n","  def __init__(self, ...):\n","    super(SkipGram, self).__init__()\n","\n","    # choose the correct class variables\n","    \"...COMPLETE HERE...\"\n","\n","  def forward(self,x):\n","    # implement this function\n","    \"...COMPLETE HERE...\"\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se_DXt7Awk8s"},"outputs":[],"source":["# make an instance of the model\n","model = SkipGram(\"...COMPLETE HERE...\")\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQ9YdBulw2eH"},"outputs":[],"source":["# function to get all the parameters\n","for i,p in enumerate(model.parameters()):\n","  print(p.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zZ4RH2CvHwc"},"outputs":[],"source":["# function to get the parameters of the model that have a name\n","for i,p in enumerate(model.named_parameters()):\n","  print(p)"]},{"cell_type":"markdown","metadata":{"id":"RelUniwMvlnD"},"source":["# Definition of the loss function\n","We want to predict the context given a word. Thus, we want to maximise the following equation:\n","$max \\prod_{center}\\prod_{context}P(context|center; \\theta)$.\n","\n","Thus, we want to minimise\n","$-min \\prod_{center}\\prod_{context}P(context|center; \\theta)$.\n","\n","By using the $log(\\cdot)$, we can sum up the elements in the product:\n","$-min \\sum_{center}\\sum_{context}log\\left(P(context|center; \\theta)\\right)$.\n","\n","\n","Now, let's define $P(context|center; \\theta)$:\n","\n","$P(context|center; \\theta)=$\n","$\\frac{exp(u^T_{context}v_{center})}{\\sum_{\\omega \\in vocab.}exp(u^T_{\\omega}v_{center})}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6K_eZqt7wIc"},"outputs":[],"source":["num_epochs = 2000\n","lr = 1e-1\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr = lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-53O1-RR7q8o"},"outputs":[],"source":["model.train()\n","for e in range(num_epochs):\n","    for x,y in zip(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0]), DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])):\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # one-hot encode input tensor\n","        input_tensor = get_input_tensor(x, vocab_size=dim_vocabulary) #shape N*V\n","        # compute the predictions\n","        y_pred = model(input_tensor)\n","        #compute loss\n","        loss = criterion(y_pred, y)\n","        # bakpropagation step\n","        loss.backward()\n","        optimizer.step()\n","\n","    if e%100 == 0:\n","        print(f'Epoch {e}, loss = {loss}')"]},{"cell_type":"markdown","metadata":{"id":"E9mYrwFw_vWV"},"source":["Let's have a look to our embeddings."]},{"cell_type":"markdown","metadata":{"id":"TuHBXSQ0mH7Y"},"source":["# Question 5\n","Plot the original embeddings and the trained ones and explain the difference between the two.\n","\n","To do so use the function `svd = decomposition.TruncatedSVD(n_components=2)` of the `scikit-learn` package.\n","\n","Using the SVD decomposition, for example using the Principal Component Analysis (PCA), we can see the embeddings in 2D vector space.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LctqL0Ik_y8o"},"outputs":[],"source":["# transform the parameters to numpy vectors\n","# using numpy() function\n","W1 = \"...COMPLETE HERE...\"\n","W2 = \"...COMPLETE HERE...\"\n","\n","# print the dimensions\n","print(W1.shape)\n","print(W2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KL27RHJSCrDe"},"outputs":[],"source":["# take the untrained matrix parameters (the untrained ones)\n","untrained_model = SkipGram(\"...COMPLETE HERE...\")\n","W1_untrained = \"...COMPLETE HERE...\"\n","W2_untrained = \"...COMPLETE HERE...\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hD37PYU8C_sF"},"outputs":[],"source":["from sklearn import decomposition\n","import seaborn as sns\n","\n","svd = decomposition.TruncatedSVD(n_components=2)\n","W1_untrained_dec = svd.fit_transform(W1_untrained)\n","x = \"...COMPLETE HERE...\"\n","y = \"...COMPLETE HERE...\"\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_untrained_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eM4m7QJ0_9vA"},"outputs":[],"source":["from sklearn import decomposition\n","import seaborn as sns\n","\n","svd = decomposition.TruncatedSVD(n_components=2)\n","W1_dec = svd.fit_transform(W1)\n","x = \"...COMPLETE HERE...\"\n","y = \"...COMPLETE HERE...\"\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXHuWwU9A3v8"},"outputs":[],"source":["W2_dec = svd.fit_transform(W2)\n","x = \"...COMPLETE HERE...\"\n","y = \"...COMPLETE HERE...\"\n","plot1 = sns.scatterplot(x=x, y=y)\n","for i in range(0,W2_dec.shape[0]):\n","     plot1.text(x[i], y[i], list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPQPykOhQhdClFwd+ug5OKS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
