{"cells":[{"cell_type":"markdown","source":["# General Note\n","In order to guide you through the homework, we put \"...COMPLETE HERE...\" as placeholder for you to complete the homework."],"metadata":{"id":"qODkBPt5Tl0J"}},{"cell_type":"markdown","metadata":{"id":"4keMCu-heF2w"},"source":["# Word-embeddings\n","Word embeddings are very important in Natural Language Processing (NLP). Ready-to-use solutions (e.g., *Glove* and many others) are useful and relatively efficient.\n","*Skip-gram* works *empirically worse* than others types of word-embeddings. Moreover, it needs much time to fit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAfNtlvsdy1-"},"outputs":[],"source":["#https://pythonspot.com/nltk-stop-words/\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import pandas as pd\n","\n","import nltk\n","from nltk.corpus import stopwords\n","import string\n","import re\n","\n","import seaborn as sns"]},{"cell_type":"markdown","source":["# Cloning the git repository\n","We clone the git repository mainly to have the folder of data inside."],"metadata":{"id":"3rSz_6WT0fJO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4CdNIsQ7l25"},"outputs":[],"source":["!git clone https://github.com/sarafrr/pytorch-tutorial-nlp-public.git"]},{"cell_type":"markdown","source":["After moving out the data repository, let's remove it."],"metadata":{"id":"wPvirqpw0ozS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHBC8M7l73Ln"},"outputs":[],"source":["!rm -r pytorch-tutorial-nlp-public"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1dUhVbsQrFy"},"outputs":[],"source":["# from the folder containing the data, obtain the list of all the files\n","from glob import glob\n","file_list = glob( \"/content/data/Shakespeare/*.txt\")\n","\n","print(file_list)"]},{"cell_type":"markdown","metadata":{"id":"-E-kSiqUfQ-A"},"source":["Skip-gram model tries to *predict context given a word*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd0ks7wJ8ONY"},"outputs":[],"source":["# download stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dH4x25s79GSn"},"outputs":[],"source":["stop_words = set(stopwords.words('english'))# e.g.: a, in, is\n","print(stop_words)\n","punctuation = set(string.punctuation)\n","print(punctuation)"]},{"cell_type":"code","source":["# import nltk.data\n","# text = '''\n","# Punkt knows that the periods in Mr. Smith and Johann S. Bach\n","# do not mark sentence boundaries.  And sometimes sentences\n","# can start with non-capitalized words.  i is a good variable\n","# name.\n","# '''\n","# sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n","# out = sent_detector.tokenize(text.strip())\n"],"metadata":{"id":"ZpzwPeGc114r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(type(out))\n","# out[:3]"],"metadata":{"id":"5kCJc4QC2F33"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEv5F15YBukf"},"outputs":[],"source":["def prepare_corpus(file_list : list, min_size : int = 1):\n","  ''' Read and prepare the corpus as a list of sentences '''\n","  # retrieve all the text from the files\n","  # a string we named complete_text\n","  complete_text = ''\n","  for p in file_list:\n","    with open(p, 'r') as f:\n","      lines = f.readlines()\n","      for l in lines:\n","        complete_text += l\n","\n","  sentences = re.split('[.,!?\\\\n]', complete_text)\n","  # retrieve the sentences by the complete_text\n","  # the correct way to get the sentences, however it takes a lot\n","  # of time\n","  #sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n","  #sentences = sent_detector.tokenize(complete_text.strip())\n","  # filter the sentences by the number of words\n","  corpus = []\n","  # sentences is a list of strings\n","  for s in sentences:\n","    if len(s.split()) > min_size*2 + 1:\n","      corpus.append(s)\n","  return corpus\n","\n","def preprocess(corpus):\n","  ''' Remove the stopwords and the punctuation marks'''\n","  processed = []\n","  for i in corpus:\n","      tokenized = nltk.word_tokenize(i)\n","      # make capital letters lowercase\n","      tokenized = [x.lower() for x in tokenized]\n","      tokenized = [x for x in tokenized if x not in (stop_words|punctuation)]\n","      processed.append(\" \". join(tokenized))\n","  return processed\n","\n","corpus = prepare_corpus(file_list=file_list, min_size = 2)\n","processed_corpus = preprocess(corpus)\n","\n","n_sentences = 2\n","print('Original corpus')\n","print(corpus[:n_sentences])\n","print('Processed corpus')\n","print(processed_corpus[:n_sentences])"]},{"cell_type":"markdown","metadata":{"id":"0TmJE3br_geO"},"source":["Create the vocabulary on the processed corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8kefLU7_elA"},"outputs":[],"source":["vocab = set()\n","for l in processed_corpus:\n","  words = l.split()\n","  vocab |= set(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cuz_nVYhjczQ"},"outputs":[],"source":["def w_to_i(vocab : set) -> dict:\n","  ''' From set to dict '''\n","\n","  vocab = dict(zip(vocab, range(0,len(vocab))))\n","  return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pltnaiL5uNeI"},"outputs":[],"source":["vocab = w_to_i(vocab)\n","print(vocab)\n","dim_vocab = len(vocab)\n","print(f'The vocabulaty has dimension {dim_vocab}')"]},{"cell_type":"markdown","metadata":{"id":"oxxT3dcnj5DL"},"source":["# Question 1\n","Try to choose the more appropriate dimension of the context to create the data, that is the paramerer `n_gram` in the function `prepare_set()` and explain why you choose for that dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAcnE8yuAAfW"},"outputs":[],"source":["from tqdm import tqdm\n","\n","def prepare_set(corpus, n_gram = 2):\n","    '''Creates a dataset with Input column and Output column for neighboring words.'''\n","    columns = ['Input', 'Output']\n","    result = pd.DataFrame(columns = columns)\n","    for sentence in tqdm(corpus):\n","      words = sentence.split()\n","      for i,w in enumerate(words):\n","          inp = w\n","          for n in range(1,n_gram+1):\n","              # look back\n","              if (i-n)>=0:\n","                  out = words[i-n]\n","                  row = pd.DataFrame([[inp,out]], columns = columns)\n","                  result = pd.concat([result,row], axis = 0, ignore_index = True)\n","\n","              # look forward\n","              if (i+n)<len(words):\n","                  out = sentence.split()[i+n]\n","                  row = pd.DataFrame([[inp,out]], columns = columns)\n","                  result = pd.concat([result,row], axis = 0, ignore_index = True)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSyrEneBx_2x"},"outputs":[],"source":["n_gram = '''COMPLETE HERE''' # context size\n","train_emb = prepare_set(processed_corpus, n_gram)\n","train_emb.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcKOpBqH-K_C"},"outputs":[],"source":["# from words to indices\n","# function map to apply to Pandas DataFrames\n","train_emb.Input = train_emb.Input.map(vocab)\n","train_emb.Output = train_emb.Output.map(vocab)\n","print(train_emb.head(10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMJqBy-JAU1z"},"outputs":[],"source":["batch_size = 10\n","train_loader_inp = DataLoader(train_emb.Input.values, batch_size=batch_size)\n","train_loader_out = DataLoader(train_emb.Output.values, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1695895163015,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"},"user_tz":-120},"id":"uUXDkq-Gy2Az","outputId":"dcd25a64-099d-42b1-8178-f5575fb865a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10])\n"]}],"source":["# check that the train_loader has the correct dimension\n","for i,x in enumerate(train_loader):\n","  print(x.shape)\n","  break"]},{"cell_type":"markdown","metadata":{"id":"IYF1nh8Px9z1"},"source":["In this way, we have represented through numbers both the input and the output. Then, we have to transform this representation of the input and the output with the one-hot-encoding representation.\n","\n","This means that the intput and the output have to be represented as vectors of the same dimension of the vocabulary, where all the elements in the vectors are zero, except for the one which represents the word.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZyDuUMi5Ki1"},"outputs":[],"source":["def get_input_tensor(indices : list, vocab_size : int):\n","    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n","    # batch size, vocab size\n","    batch_size = indices.size(0)\n","    # scatter_(dim, index, src)\n","    # index is giving the indices along the rows (dim=1) where to modify the tensor that\n","    # is the caller, src is giving the elements to insert when modifying the\n","    # tensor\n","    if indices.is_cuda:\n","      base_matrix = torch.zeros(batch_size, vocab_size).to('cuda:0')\n","      one_hot_encoded_input = base_matrix.scatter_(1,indices.unsqueeze(1),1.)\n","    else:\n","      one_hot_encoded_input = torch.zeros(batch_size, vocab_size).scatter_(1,indices.unsqueeze(1),1.)\n","    # print(one_hot_encoded_input)\n","    return one_hot_encoded_input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIvWrPLg1z11"},"outputs":[],"source":["# check that the function to obtain the one-hot-encoding is correclty\n","# working\n","for i,x in enumerate(train_loader):\n","  print(x.size())\n","  x = x.to('cuda:0')\n","  print(x.device)\n","  tmp = get_input_tensor(x, vocab_size=dim_vocab)\n","  print(tmp.device)\n","  break"]},{"cell_type":"code","source":["# another useful function : get the list from a tensor\n","print(tmp.tolist())\n","tmp.size()\n","# generally you use item() for the getting the error value from the loss\n","# however it is automatically converted to a list, indeed you can\n","# have the element with the tolist() function\n","tmp1 = torch.Tensor(1)\n","elem = tmp1.item()\n","elem1 = tmp1.tolist()\n","\n","print(elem)\n","print(elem1)"],"metadata":{"id":"Irk3vfri_CZ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rC8KRY9ru_uy"},"source":["In between the input and output, there is an hidden layer we choose. The *length of the hidden layer* gives the *dimension of the embedding vectors*.\n","\n","The most interesting part of this network are the *weights in between the hidden layer and the two other layers*: the input and the output.\n","\n","The multiplication of the one-hot encoded vector with the matrix of weights will activate the only row which corresponds to the $1$ in the input vector.\n"]},{"cell_type":"markdown","metadata":{"id":"LZGXoDCOkqZ8"},"source":["# Question 2\n","Implement SkipGram model by defining an appropriate class which extends `nn.Module` and train it on the data.\n","\n","# Question 3\n","Variate the embedding dimension in the range `{5,10,100}` and try to choose the best embedding dimension. Explain why you chose that dimension.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIPGHI6U5X3S"},"outputs":[],"source":["dim_embedding = '''COMPLETE HERE'''\n","\n","device = torch.device('cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4M9GYeDvuYA"},"outputs":[],"source":["import torch.nn as nn\n","\n","class SkipGram(nn.Module):\n","  def __init__(self,'''COMPLETE HERE'''):\n","    super(SkipGram, self).__init__()\n","    '''COMPLETE HERE'''\n","\n","  def forward(self,x):\n","    '''COMPLETE HERE'''\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se_DXt7Awk8s"},"outputs":[],"source":["model = SkipGram('''COMPLETE HERE''')\n","\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQ9YdBulw2eH"},"outputs":[],"source":["for i,p in enumerate(model.parameters()):\n","  print(p.shape)"]},{"cell_type":"markdown","metadata":{"id":"RelUniwMvlnD"},"source":["# Definition of the loss function\n","We want to predict the context given a word. Thus, we want to maximise the following equation:\n","$max \\prod_{center}\\prod_{context}P(context|center; \\theta)$.\n","\n","Thus, we want to minimise\n","$-min \\prod_{center}\\prod_{context}P(context|center; \\theta)$.\n","\n","By using the $log(\\cdot)$, we can sum up the elements in the product:\n","$-min \\sum_{center}\\sum_{context}log\\left(P(context|center; \\theta)\\right)$.\n","\n","\n","Now, let's define $P(context|center; \\theta)$:\n","\n","$P(context|center; \\theta)=$\n","$\\frac{exp(u^T_{context}v_{center})}{\\sum_{\\omega \\in vocab.}exp(u^T_{\\omega}v_{center})}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6K_eZqt7wIc"},"outputs":[],"source":["num_epochs = 10\n","lr = 1e-1\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n","mmodel = model.to('cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-53O1-RR7q8o"},"outputs":[],"source":["model.train()\n","for e in range(num_epochs):\n","    for x,y in zip(train_loader_inp, train_loader_out):\n","        x = x.to('cuda:0')\n","        y = y.to('cuda:0')\n","\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # one-hot encode input tensor\n","        input_tensor = get_input_tensor(x, vocab_size=dim_vocab)\n","\n","        # compute the predictions\n","        y_pred = model(input_tensor)\n","        #compute loss\n","        loss = criterion(y_pred, y)\n","        # bakpropagation step\n","        loss.backward()\n","        optimizer.step()\n","\n","    if e%1 == 0:\n","        print(f'Epoch {e}, loss = {loss}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgjRX38Rx4bN"},"outputs":[],"source":["# function to get the parameters of the model that have a name\n","for i,p in enumerate(model.named_parameters()):\n","  print(p)"]},{"cell_type":"markdown","metadata":{"id":"E9mYrwFw_vWV"},"source":["Let's have a look to our embeddings."]},{"cell_type":"markdown","metadata":{"id":"TuHBXSQ0mH7Y"},"source":["# Question 4\n","Plot the original embeddings and the trained ones and explain the difference between the two.\n","\n","To do so use the function `tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3)` of the `scikit-learn` package.\n","\n","Indeed, using T-distributed Stochastic Neighbor Embedding (or other similar functions, such as the PCA), we can see the embeddings in 2D vector space. Explain the results considering the best plot you obtain by one of the two functions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LctqL0Ik_y8o"},"outputs":[],"source":["W1 ='''COMPLETE HERE'''\n","W2 = '''COMPLETE HERE'''\n","\n","print(W1.shape)\n","print(W2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KL27RHJSCrDe"},"outputs":[],"source":["orig_model = SkipGram(dim_voc=dim_vocab, emb_dim=dim_embedding)\n","# here we did not pass into the gpu, thus we do not have to pass the matrices\n","# angain in the cpu\n","W1_orig = '''COMPLETE HERE'''\n","W2_orig = '''COMPLETE HERE'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQqQKzLwBMm9"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","tsne = TSNE(n_components=2, learning_rate='auto',\n","                  init='random', perplexity=3)\n","W1_orig_dec = tsne.fit_transform(W1_orig)\n","x = '''COMPLETE HERE'''\n","y = '''COMPLETE HERE'''\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_orig_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocab.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_5e3PIMB-xO"},"outputs":[],"source":["tsne = TSNE(n_components=2, learning_rate='auto',\n","                  init='random', perplexity=3)\n","W1_dec = tsne.fit_transform(W1)\n","x = '''COMPLETE HERE'''\n","y = '''COMPLETE HERE'''\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_orig_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocab.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yr0jPGtLCPjL"},"outputs":[],"source":["tsne = TSNE(n_components=2, learning_rate='auto',\n","                  init='random', perplexity=3)\n","W2_dec = tsne.fit_transform(W2)\n","x = '''COMPLETE HERE'''\n","y = '''COMPLETE HERE'''\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_orig_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocab.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"markdown","metadata":{"id":"5hHPU7b0CIIL"},"source":["Or using the PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hD37PYU8C_sF"},"outputs":[],"source":["from sklearn import decomposition\n","\n","svd = decomposition.TruncatedSVD(n_components=2)\n","W1_orig_dec = svd.fit_transform(W1_orig)\n","x = '''COMPLETE HERE'''\n","y = '''COMPLETE HERE'''\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_orig_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocab.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eM4m7QJ0_9vA"},"outputs":[],"source":["from sklearn import decomposition\n","import seaborn as sns\n","\n","svd = decomposition.TruncatedSVD(n_components=2)\n","W1_dec = svd.fit_transform(W1)\n","x = '''COMPLETE HERE''']\n","y = '''COMPLETE HERE'''\n","plot = sns.scatterplot(x=x, y=y)\n","\n","for i in range(0,W1_dec.shape[0]):\n","     plot.text(x[i], y[i], list(vocab.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXHuWwU9A3v8"},"outputs":[],"source":["W2_dec = svd.fit_transform(W2)\n","x = '''COMPLETE HERE'''\n","y = '''COMPLETE HERE'''\n","plot1 = sns.scatterplot(x=x, y=y)\n","for i in range(0,W2_dec.shape[0]):\n","     plot1.text(x[i], y[i], list(vocab.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNnelkpYR1z1x8Vd+j5Vy0y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}