{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# General Note\n","In order to guide you through the homework, we put \"...COMPLETE HERE...\" as placeholder for you to complete the homework."],"metadata":{"id":"MMeJPItPzCsG"}},{"cell_type":"markdown","source":["# Word Embeddings\n","Word embeddings are *dense vectors of real numbers* (they have tipically entries that are non-zero). One dense vector per element in a dictionary.\n","In Natural Language Processing (NLP) you deal with words.\n","\n","Thus, you have to represent words in a computer, how to do it?\n","It is possible to use the ASCII representation of the characters composing the word.\n","\n","However, this kind of representation does not tell you anything about its meaning, but only about what it is exactly.\n","\n","Generally you want to obtain a sort of representation of your high-dimensional data to a smaller representation.\n","\n","You can also usa the so-called ''one-hot encoding'' of the word in a vocabulary. Let's represent the word $w$ in a vocabulary of $V$ words:\n","\n","\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n","\n","where we have $1$ at the position of $w$.\n","\n","There are different drawbacks in representing the word like that. It could be an enormous representation, but also it is considering the words as independent. We would like to have a representation of the words that contains a sort of notion of *similarity*.\n","\n","Let's see why.\n","\n","Suppose we want to build a language model, and that we have seen the following sentences:\n","\n","\n","*   the girl ran to the shop\n","*   the boy ran to the shop\n","*   the girl bought a flower\n","\n","in the training data. Then, suppose we see sentences never seen before in our training data\n","\n","*   the boy bought a flower\n","\n","Maybe, the language model would tell that it is probable the previous sentence. But what about using the following information we have seen in our training data:\n","*  we have seen the boy and the girl in the same role in the sentence, thus maybe they share some semantic information\n","*  we have seen the girl in the same relation of the boy we are seeing now\n","\n","These two facts shold increase the probability of correctness of the sentences.\n","We mean this for *semantic similarity*. This is a technique to combat the sparsity of the linguistic data.\n","\n","This representation however rely on the assumption that words appearning in similar contexts are related to each other semantically.\n","\n","\n","## Getting Dense Word Embeddings\n","How could we encode semantic similarity in words? Maybe, we can introduce some semantic attributes.\n","\n","For example, we know that both the boy and girl can run. Thus, we can give an high score to the attribute \"is able to run\". We can think other semantic attributes and in this way we construct the embeddings of the two subjects: the boy and the girl.\n","\n","\\begin{align}q_\\text{boy} = \\left[ \\overbrace{2.3}^\\text{can run},\n","   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{likes beach}, \\dots \\right]\\end{align}\n","\n","\\begin{align}q_\\text{girl} = \\left[ \\overbrace{2.5}^\\text{can run},\n","   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{likes beach}, \\dots \\right]\\end{align}\n","\n","We can measure the similarity between the boy and girl by doing a dot-product between the two representations:\n","\n","\\begin{align}\\text{Similarity}(\\text{boy}, \\text{girl}) = q_\\text{boy} \\cdot q_\\text{girl}\\end{align}\n","\n","Although it is more common to normalize by the lengths:\n","\n","\\begin{align}\\text{Similarity}(\\text{boy}, \\text{girl}) = \\frac{q_\\text{boy} \\cdot q_\\text{girl}}\n","   {\\| q_\\text{boy} \\| \\| q_\\text{girl} \\|} = \\cos (\\phi)\\end{align}\n","\n","Where $\\phi$ is the angle between the two vectors. That way,\n","extremely similar words (words whose embeddings point in the same\n","direction) will have similarity 1. Extremely dissimilar words should\n","have similarity -1.\n","\n","\n","We let the network learn this semantic attributes, by definining them as parameters. We will have some *latent semantic attributes* that the network can, in principle, learn. Note that the word embeddings will probably not be interpretable.\n","\n","In summary, **word embeddings are a representation of the *semantics* of\n","a word, efficiently encoding semantic information that might be relevant\n","to the task at hand**.\n","\n","## Word Embeddings in Pytorch\n","\n","Similarly to what we do with the one-hot-encoding representation, we have to define an index for each word in the vocabulary. These will be the keys in a lookup table. That is embeddings are stored in a $|V| \\times D$ matrix, were $D$ is the dimensionality of the embeddings, such that the word assigned inex $i$ has the embedding stored in the row of the matrix of index $i$.\n","\n","The module that permits you to do the embeddings is torch.nn.Embedding which takes to parameters: the dimensionality of the vocabulary and the dimension of the embeddings.\n","\n","\n","\n"],"metadata":{"id":"4i8ua3T7fN-b"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import spacy\n","import numpy as np\n","import random"],"metadata":{"id":"FILi19R_rrBH","executionInfo":{"status":"ok","timestamp":1695804387790,"user_tz":-120,"elapsed":21523,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# this is all the necessary code to set the seed\n","def set_seed(seed : int = 123):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"oVmcI1m6FP-4","executionInfo":{"status":"ok","timestamp":1695804387791,"user_tz":-120,"elapsed":8,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["set_seed()"],"metadata":{"id":"4HRsafGhFdK-","executionInfo":{"status":"ok","timestamp":1695804387791,"user_tz":-120,"elapsed":7,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["word_to_ix = {\"hello\": 0, \"world\": 1}\n","\n","# rows = n. words in the vocabulary\n","# cols = dimension of the embedding\n","embeds = nn.Embedding(2, 5)\n","# tensor to store the index to access the embedding of a word\n","word = 'hello'\n","# word = 'world'\n","lookup_tensor = torch.tensor([word_to_ix[word]], dtype=torch.long)\n","\n","word_embed = embeds(lookup_tensor)\n","print(word_embed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0p26mHrrxZx","outputId":"4947bf2d-a718-4863-9593-3b3cae95697f","executionInfo":{"status":"ok","timestamp":1695804387791,"user_tz":-120,"elapsed":6,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"code","source":["# load google drive to see the files in google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnawCyXitAeR","outputId":"1ffc5a9d-9ab6-4719-f449-8dff5292bed6","executionInfo":{"status":"ok","timestamp":1695758568869,"user_tz":-120,"elapsed":19998,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!wget #link from where to download the file"],"metadata":{"id":"VqYBEoufxfxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise: Compute the Word Embedding Continuous Bag-of-Words\n","\n","The Continuous Bag-of-Words (CBOW) is widely used in NLP, it is a way to embed the context of few words before and after of the target word.\n","\n","CBOW is used to train word embeddings and these embeddings are used as initialisation of a more complicate model.\n","\n","This is usually referred as *pretraining embeddings*.\n","\n","Given a target word $w_i$ and an\n","$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n","and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n","collectively as $C$, CBOW tries to minimize\n","\n","\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right)\\end{align}\n","\n","where $q_w$ is the embedding for word $w$.\n","\n","\n","Implement the CBOW class in the following snippet of code and train it."],"metadata":{"id":"t-6tHBiFBtv8"}},{"cell_type":"code","source":["# 2 words to the left and two to the right\n","CONTEXT_SIZE = 2\n","EMBEDDING_SIZE = 100\n","EPOCHS = 50\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"zb9_VJd8lbY3","executionInfo":{"status":"ok","timestamp":1695804402279,"user_tz":-120,"elapsed":246,"user":{"displayName":"SARA FERRO","userId":"01101448379566877052"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def create_dataset():\n","  # read the text file raw_text.txt as utf-8 in a string\n","  with open('/content/drive/MyDrive/Colab Notebooks/data/anna.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","    # tokenize raw text with spacy\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    text = text[:nlp.max_length//10]\n","\n","    tokenized_text = [token.text for token in nlp(text)] # text, pos_, dep_\n","    # to sort the tokens is not necessary\n","    vocab = sorted(set(tokenized_text))\n","\n","    w_2_idx = {w:i for i,w in enumerate(vocab)}\n","    idx_2_w = {i:w for i,w in enumerate(vocab)}\n","\n","    # create the data composed by the tuple of the context and the target\n","    data = []\n","    for i in range(CONTEXT_SIZE, len(tokenized_text)-CONTEXT_SIZE):\n","\n","      context_idx = [w_2_idx[tokenized_text[i-j-1]] for j in range(CONTEXT_SIZE)]+ \\\n","              [w_2_idx[tokenized_text[i+j+1]] for j in range(CONTEXT_SIZE)]\n","      target_idx = w_2_idx[tokenized_text[i]]\n","\n","      data.append((context_idx, target_idx))\n","    return data, w_2_idx, idx_2_w"],"metadata":{"id":"j4zI7dfcr4Y2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 1\n","Implement CBOW model by defining an appropriate class which extends `nn.Module` and train it on the data."],"metadata":{"id":"7W4IAF-VwtSL"}},{"cell_type":"code","source":["# Create the model\n","class CBOW(nn.Module):\n","\n","    def __init__(self, \"...COMPLETE HERE...\"):\n","        super(CBOW, self).__init__()\n","        \"...COMPLETE HERE...\"\n","\n","    def forward(self, x):\n","        \"...COMPLETE HERE...\"\n","        return x"],"metadata":{"id":"OpKprQAeDQrn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 2\n","Choose the correct loss and explain why you chose that loss.\n","\n","# Question 3\n","Make an exaple of prediction and see that the prediction is correct. See the end of the following script."],"metadata":{"id":"BXWkdUhxxDBn"}},{"cell_type":"code","source":["\n","data, w_2_idx, idx_2_word = create_dataset()\n","# implement the loss\n","# explain why you used the loss you chose\n","criterion = \"...COMPLETE HERE...\"\n","model = CBOW(\"...COMPLETE HERE...\")\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# create the context data and targets in tensors\n","context_data = torch.tensor([d[0] for d in data])\n","target_data = torch.tensor([d[1] for d in data])\n","\n","# create a dataset and a dataloader\n","dataset = torch.utils.data.TensorDataset(context_data, target_data)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","for e in range(EPOCHS):\n","  for idx, data in enumerate(dataloader):\n","    context = data[0]\n","    target = data[1]\n","    predicted = model(context)\n","    loss = criterion(predicted, target)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","  print(f'Epoch {e} - loss: {loss.item()}')\n","\n","# --- TESTING ---\n","data, w_2_idx, idx_2_word = create_dataset()\n","# create the context data and targets in tensors\n","context_data = torch.tensor([d[0] for d in data])\n","target_data = torch.tensor([d[1] for d in data])\n","\n","# create a dataset and a dataloader\n","dataset = torch.utils.data.TensorDataset(context_data, target_data)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","for idx, data in enumerate(dataloader):\n","  context = data[0]\n","  target = data[1]\n","  out = model(context)\n","  break\n","\n","#print result\n","print(f'Context: {\"...COMPLETE HERE...\"}')\n","print(f'Target: {\"...COMPLETE HERE...\"}')\n","print(f'Prediction: {\"...COMPLETE HERE...\"}')\n"],"metadata":{"id":"mqPGzXCvxf54"},"execution_count":null,"outputs":[]}]}